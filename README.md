# AI Research Paper Reading List

Welcome to my curated list of research papers in the field of Artificial Intelligence. Explore cutting-edge papers on various AI topics.

## Table of Contents
| S.N | Topics |
| ---- | ---- |
| 1 | [Computer Vision](#computer-vision) |
| 2 | [Large Language Model](#large-language-model) |
## Computer Vision
| S.N | Paper | Resources | Abstract | Note Link |
| ---- | ---- | ---- | ---- | ---- |
| 1 | MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications |  |  |  |

## Large Language Model
| S.N | Paper | Resources | Year of Release | Abstract | Note Link |
| ---- | ---- | ---- | :--: | :--- | ---- |
| 1 | Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity | - [Paper](https://arxiv.org/pdf/2101.03961.pdf) | 2022 | The research paper presents challenges in deep learning models, specifically Mixture of Experts (MoE) models, known for selecting different parameters for each input. Despite successful applications, MoE adoption faces hindrances such as complexity, communication costs, and training instability. The Switch Transformer is introduced as a solution, simplifying MoE routing algorithms and designing improved models with reduced computational and communication costs. Proposed training techniques address instabilities, enabling the training of large sparse models with lower precision. Models based on T5-Base and T5-Large demonstrate up to 7x pre-training speed increases, with extensions to multilingual settings showing gains across 101 languages. Furthermore, the researchers scale language models to trillion parameters, achieving a 4x speedup over the T5-XXL model on the "Colossal Clean Crawled Corpus." | [link](./llm/switch-transformers/README.md) |
|  |  |  |  |  |  |

## Contribution Guidelines
Feel free to contribute by adding new papers or suggesting improvements. 

